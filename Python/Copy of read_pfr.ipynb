{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9RtNuCT6LMzXD4f0TxsK4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vGqHNdZnVvtp","executionInfo":{"status":"ok","timestamp":1666133370015,"user_tz":240,"elapsed":11692,"user":{"displayName":"Michael Golaski","userId":"16745854907775039375"}},"outputId":"af611caa-2312-4f31-e72c-cd6cd8e8489a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting selenium\n","  Downloading selenium-4.5.0-py3-none-any.whl (995 kB)\n","\u001b[K     |████████████████████████████████| 995 kB 4.3 MB/s \n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Collecting trio~=0.17\n","  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n","\u001b[K     |████████████████████████████████| 384 kB 42.9 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n","Collecting urllib3[socks]~=1.26\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 16.1 MB/s \n","\u001b[?25hCollecting exceptiongroup>=1.0.0rc9\n","  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Collecting outcome\n","  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n","Collecting sniffio\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n","Installing collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, urllib3, trio, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n","Successfully installed async-generator-1.10 exceptiongroup-1.0.0rc9 h11-0.14.0 outcome-1.2.0 selenium-4.5.0 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.12 wsproto-1.2.0\n"]}],"source":["import pandas as pd\n","import numpy as np\n","!pip install selenium\n","from selenium import webdriver\n","from lxml import etree\n","import time\n","import json\n","import os\n","\n","\n","# Make a dictionary to store what three-letter code each \n","# team used in a given season\n","three_letter_code = {}\n","# Most teams don't change, but some do\n","tms = [\n","    'atl','nor','car','tam','nyg','dal','phi','was',\n","    'min','gnb','det','chi','sea','sfo','crd','ram',\n","    'pit','cle','cin','rav','buf','nyj','mia','nwe',\n","    'jax','htx','oti','clt','den','kan','rai','sdg',\n","    'lac'\n","]\n","tlcs = {k:[k] for k in tms}\n","tlcs['crd'].append('ari')\n","tlcs['ram'].append('lar')\n","tlcs['ram'].append('stl')\n","tlcs['rav'].append('bal')\n","tlcs['oti'].append('ten')\n","tlcs['clt'].append('ind')\n","tlcs['htx'].append('hou')\n","tlcs['rai'].append('oak')\n","tlcs['sdg'].append('lac')\n","tlcs['lac'].append('sdg')\n","for tm in tms:\n","    for yr in range(2000,2022):\n","        k = '{}{}'.format(tm,yr)\n","        three_letter_code[k] = tm\n","# Arizona Cardinals\n","for yr in range(2000,2022):\n","    k = 'crd{}'.format(yr)\n","    three_letter_code[k] = 'ari'\n","# St Louis Rams\n","    # 2016-17 - lar\n","    # 2000-15 - stl\n","three_letter_code['ram2022'] = 'lar'\n","three_letter_code['ram2021'] = 'lar'\n","three_letter_code['ram2020'] = 'lar'\n","three_letter_code['ram2019'] = 'lar'\n","three_letter_code['ram2018'] = 'lar'\n","three_letter_code['ram2017'] = 'lar'\n","three_letter_code['ram2016'] = 'lar'\n","for yr in range(2000,2016):\n","    k = 'ram{}'.format(yr)\n","    three_letter_code[k] = 'stl'\n","# Baltimore Ravens\n","for yr in range(2000,2022):\n","    k = 'rav{}'.format(yr)\n","    three_letter_code[k] = 'bal'\n","# Oilers/Titans\n","for yr in range(2000,2022):\n","    k = 'oti{}'.format(yr)\n","    three_letter_code[k] = 'ten'\n","# Indianapolis Colts\n","for yr in range(2000,2022):\n","    k = 'clt{}'.format(yr)\n","    three_letter_code[k] = 'ind'\n","# Texans\n","for yr in range(2000,2022):\n","    k = 'htx{}'.format(yr)\n","    three_letter_code[k] = 'hou'\n","# Raiders\n","for yr in range(2000,2022):\n","    k = 'rai{}'.format(yr)\n","    three_letter_code[k] = 'oak'\n","# Chargers\n","three_letter_code['sdg2022'] = 'lac'\n","three_letter_code['sdg2021'] = 'lac'\n","three_letter_code['sdg2020'] = 'lac'\n","three_letter_code['sdg2019'] = 'lac'\n","three_letter_code['sdg2018'] = 'lac'\n","three_letter_code['sdg2017'] = 'lac'\n","for yr in range(2000,2016):\n","    k = 'sdg{}'.format(yr)\n","    three_letter_code[k] = 'sdg'\n","\n","\n","def read_table( html_tree, \n","                tablename ):\n","    \n","    # Make list to house dictionaries for each row\n","    rows = []\n","    tablepath = '//table[@id=\"{0}\"]/tbody/tr'.format(tablename)\n","    \n","    # Split table into rows\n","    for row in html_tree.xpath(tablepath):\n","        \n","        # Make a dictionary to store each cell in the row\n","        rd = {}\n","        rowclass = row.xpath('./@class')\n","        try:\n","            rd[\"rowclass\"] = rowclass[0]\n","        except:\n","            pass\n","        try:\n","            cells = [e for e in row.xpath('./td|./th')]\n","            for i, cell in enumerate(cells):\n","                \n","                # Depending on cell contents, add cell to row dict\n","                try:\n","                    txt = cell.xpath('./text()')\n","                    a_text = [x.text for x in cell.findall(\".//a[@href]\")]\n","                    a_href = [x.get(\"href\") for x in cell.findall(\".//a[@href]\")]\n","                    stat = cell.xpath('./@data-stat')\n","                    tip = cell.xpath('./@data-tip')\n","                \n","                    # Logic map for cell contents\n","                    if (len(tip) >= 1):\n","                        # Have a data-tip. This may be an injury report page\n","                        rd[stat[0]] = tip\n","                    elif (len(txt) >= 1) and (len(a_text) >= 1):\n","                        # Have both links and standard text. Save both\n","                        rd[stat[0]+\"_text\"] = \"brk, \".join(txt)\n","                        rd[stat[0]+\"_a\"] = \", \".join(a_text)\n","                        rd[stat[0]+\"_href\"] = \", \".join(a_href)\n","                    elif len(a_text) >= 1:\n","                        # Have just text from a link\n","                        rd[stat[0]+\"_a\"] = a_text[0]\n","                        rd[stat[0]+\"_href\"] = a_href[0]\n","                    else:\n","                        try:\n","                            # Maybe we just have text\n","                            rd[stat[0]] = txt[0]\n","                        except:\n","                            # If all fails, then we probably have no text\n","                            rd[stat[0]] = \"\"\n","                                        \n","                except:\n","                    print(\"Couldn't parse a cell\")\n","                    print(etree.tostring(cell, pretty_print=True))\n","            \n","            \n","            # Add row dictionary to list of rows\n","            rows.append(rd)\n","\n","        except:\n","            pass\n","        \n","    return rows\n","\n","\n","# One function to take an element tree and parse all of the tables on it\n","def get_tables(url):\n","    \n","    options = webdriver.ChromeOptions()\n","    options.add_argument('headless')\n","\n","    driver = webdriver.Chrome(chrome_options=options)\n","    try:\n","        driver.get(url)\n","        page_html = driver.page_source\n","        tree = etree.HTML(page_html)\n","        tablenames = tree.xpath('//table/@id')\n","        \n","    except:\n","        print(\"webdriver failed to get url\",url)\n","        tablenames = [\"\"]\n","    driver.quit()\n","    \n","    tables = {}\n","    for tab in tablenames:\n","        try:\n","            tables[tab] = read_table(tree, tab)\n","        except:\n","            print(\"Failed to read table\",tab)\n","            tables[tab] = \"\"\n","            \n","    return tables\n","\n","\n","def read_season_sched( season ):\n","    url = \"https://pro-football-reference.com/years/\"+str(season)+\"/games.htm\"\n","    print(\"Reading\",url)\n","    season_dict = {\"\":\"\"}\n","    tries = 1\n","    while (tries < 5) and ( season_dict == {\"\":\"\"} ):\n","        season_dict = get_tables(url)\n","        tries += 1\n","        time.sleep(0.1) \n","    return season_dict\n","\n","\n","def read_game_page( gid ):\n","    url = \"http://pro-football-reference.com\"+str(gid)\n","    print(\"Trying to read\",url)\n","    game_dict = {\"\":\"\"}\n","    tries = 1\n","    while (tries < 5) and ( game_dict == {\"\":\"\"} ):\n","        game_dict = get_tables(url)\n","        tries += 1\n","        time.sleep(0.1)\n","    return game_dict\n","\n","\n","def read_inj_page( team, season ):\n","    url = 'https://www.pro-football-reference.com/teams/{}/{}_injuries.htm'.format(team,season)\n","    print(\"Trying to read\",url)\n","    page_dict = {\"\":\"\"}\n","    tries = 1\n","    while (tries < 5) and (page_dict == {\"\":\"\"}):\n","        page_dict = get_tables(url)\n","        tries += 1\n","        time.sleep(0.1)\n","    return page_dict"]}]}